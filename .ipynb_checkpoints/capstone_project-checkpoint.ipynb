{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713be179-e0af-47b3-9661-bea4a74a3331",
   "metadata": {},
   "source": [
    "# GenAI for Humanists\n",
    "*Capstone project - 2024W 136031-1 GenAI for Humanists*\n",
    "\n",
    "***\n",
    "\n",
    "## Generative AI for Automated Text Assessment and Tailored Practice in Spanish Language Learning - *Project Proposal, Felix Aufreiter*\n",
    "\n",
    "The primary goal of this project is to create a generative AI-driven pipeline that supports Spanish language learners in improving their writing skills. This system will automatically analyse students' written texts, identifying linguistic mistakes and assessing their overall proficiency according to the *Common European Framework of Reference for Languages (CEFRL)* and the available grading schemes of the *BMBWF*.\n",
    " \n",
    "Based on this analysis, the system will generate personalised practice exercises that focus on the specific areas where each student struggles, ensuring targeted and effective learning. These new exercises will be accompanied by detailed solutions. Students will be provided with the possibility to use these newly created tasks to improve their grammar skills. \n",
    "\n",
    "Either OpenAI API or a suitable open-source LLM will be selected as the platform for developing this pipeline with detailed prompt engineering. This project should combine generative AI and humanistic education. The aim is to use generative AI to improve the personalised support of students and help educators/teachers to create personalised exercises.\n",
    "\n",
    "### A short guide to the planned development process.\n",
    "####  Data Collection\n",
    "* Collect a dataset of Spanish language learner texts.\n",
    "* As I work in adult and secondary education myself, I have access to authentic texts from students who currently have a Spanish level of around A1-A2. These texts could be used anonymously for the test of the prompts developed. \n",
    "#### Pipeline Development\n",
    "* **Step 1:** Preprocess student submissions, if necessary with OCR provided by OpenAI API or an open-source model.\n",
    "* **Step 2:** Detect specific errors (e.g., verb conjugations, word order, prepositions).\n",
    "* **Step 3:** Use the CEFRL rating scheme to classify texts into proficiency levels. The available rating schemes of the Bundesministerium für Bildung, Wissenschaft und Forschung will be used to grade the texts in accordance with the official guidelines for austrian teachers (Bundesministerium für Bildung, Wissenschaft und Forschung, n.d.).\n",
    "* **Step 4:** Provide an improved/corrected version of the text . \n",
    "* **Step 5:** Use generative AI to design practice materials targeting the identified errors. \n",
    "* **Step 6:** Convert the generated exercises to a pdf.\n",
    "* **Step 7:** Saving the personalised files to a cloud folder with the option to share the newly created files. (This step will only be done if a real-world test is possible.)\n",
    "#### Expected Outcomes\n",
    "* Corrected texts with a CEFRL rating according to the learners’ level, corrected texts according to official grading schemes of the Austrian educational system.\n",
    "* Exercises for the students (pdf with solutions for self-study).\n",
    "#### Future Use\n",
    "* The plan would be to use this project in a real-world scenario, because I work in education.\n",
    "* Furthermore, I am currently developing a relational database which consists of already used tasks for official exams (\"Aufgabenpool\" - https://aufgabenpool.at/srdp_lfs/index.php?id=sp). These tasks will be adapted to specific needs of the students for further use. Combining the projects of the course \"GenAI for Humanists\" and the relational database of the \"Research Seminar\" which also uses an LLM to create new tasks.\n",
    "*  A great idea would be to connect an LLM to the database to make the search process more userfriendly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42aaa2a-e233-4c1c-a881-fc5e6eaf7134",
   "metadata": {},
   "source": [
    "## Inicial setup\n",
    "* Step 1: importing OpenAI API-Key\n",
    "* Step 2: importing packages\n",
    "* Step 3: creating a function and test prompt to use **OpenAI API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "610122ef-df18-4aef-9ab2-31ab766ec8e9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from python-docx) (4.8.0)\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install openai python-dotenv\n",
    "!pip install python-docx\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf372861-00f2-4cf2-859c-f802946df5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Access your API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd32d2a-e621-475c-827f-a95cbaf436bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\faufr\\Documents\\GitHub\\GenAI_project\n",
      "Contents: ['.env', '.git', '.gitignore', '.ipynb_checkpoints', 'capstone_project.ipynb', 'README.md']\n"
     ]
    }
   ],
   "source": [
    "#Checking directory for '.env' + '.gitignore'\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Contents:\", os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513cc45e-a7a1-4145-a051-531ca6c7f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK your API key\n",
    "print(os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852ff85-64d8-4e17-abbd-bc951d4f1ebd",
   "metadata": {},
   "source": [
    "## Creating test-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c622484-1af1-4130-8a7a-ef5c7bc804f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a very skilled language teacher. Your answers are precise and you use easy language.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e621faf-bfca-4188-b8ce-687fec17e173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Undertaker is a famous professional wrestler in WWE (World Wrestling Entertainment). His real name is Mark Calaway, and he is known for his dark, mysterious character and impressive performances in the ring. He has been a part of wrestling for over 30 years and is considered one of the greatest wrestlers of all time.\n",
      "\n",
      "Some of his favorite wrestling moves include:\n",
      "\n",
      "1. **Tombstone Piledriver**: This is his signature finishing move, where he lifts his opponent upside down and then drops them on their head.\n",
      "\n",
      "2. **Chokeslam**: He grabs his opponent by the throat and lifts them high before slamming them down to the mat.\n",
      "\n",
      "3. **Last Ride**: This is a powerful move where he lifts his opponent onto his shoulders and then slams them down.\n",
      "\n",
      "4. **Old School**: He walks along the top rope and then jumps down onto his opponent.\n",
      "\n",
      "These moves are part of what makes The Undertaker a legendary figure in wrestling!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who is 'The Undertaker' and what are his favorite wrestling moves?\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1999af-b388-43ad-9d74-b3ddf42b98f4",
   "metadata": {},
   "source": [
    "## Importing texts\n",
    "* example texts to analyze with the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d32ea029-c0e0-4302-84fc-c4c8f85b817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<docx.document.Document object at 0x00000187AA941220>\n",
      "Correo electronico \n",
      "\n",
      "De: adrian123@gmail.com\n",
      "Para: padres123@gmail.com\n",
      "Fecha: 12 de novembre de 2024\n",
      "Asunto: me ciudad favorita\n",
      "\n",
      "Queridos padres,\n",
      "\n",
      "Estoy en mi ciuadad favorita, Barcelona. La ciudad es muy bonita y hay mucha gente viviendo aquí. Además, hace mucho calor aquí en Barcelona.\n",
      "\n",
      "Hoy he visto la ciudad. Para el almuerzo probe tapas en un pequeno restaurante local. ! Fue delicioso! Me gusta de la playa y comer. Luego pasee por el parque y pasee por la playa de la noche. Mas tarde me relaje en la playa de la Barcelona.\n",
      "\n",
      "Manana tengo planes emocionantes. Quiero visitar muchos mercados y restaurantes para probar mas comida tipica. Tambien espero encontar algunos Souvenirs unicos para llevar a casa.\n",
      "\n",
      "Estoy disfrutando mucho de mi tiempo aqui y no puedo esperar para contarles mas.\n",
      "\n",
      "Saludos,\n",
      "Adrian\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#IMPORTING example text\n",
    "from docx import Document\n",
    "\n",
    "# Path to the Word document\n",
    "file_path = './data/correo electronico.docx'\n",
    "\n",
    "# Open and read the document\n",
    "doc = Document(file_path)\n",
    "print(doc)\n",
    "\n",
    "# Combine all paragraphs into one string\n",
    "document_text = \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
    "print(document_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a879a93d-3657-4b7c-9c45-f6c6151528f0",
   "metadata": {},
   "source": [
    "## Analyzing the data with an LLM\n",
    "\n",
    "* The prompt was designed using the `CO-STAR framework`. (https://towardsdatascience.com/how-i-won-singapores-gpt-4-prompt-engineering-competition-34c195a93d41)\n",
    "***\n",
    "    * Context: Provide background information on the task\n",
    "    * Objective: Define what the task is that you want the LLM to perform\n",
    "    * Style: Specify the writing style you want the LLM to use\n",
    "    * Tone: Set the attitude of the response\n",
    "    * Audience: Identify who the response is intended for\n",
    "    * Response: Provide the response format\n",
    "***\n",
    "* The aim of this prompt is to provide examples structure for the LLM to correct the provided text sample (\"correo_electrónico\" --> document_text). The LLM should use the CEFRL (Common Framework of Reference for Language) to set a bar for the expected quality of the written text which was delivered by a student.\n",
    "* The actual structre of the feedback is derived from the offical Austrian guidelines for grading students' assignements released by the Austrian **Ministry for Education, Science and Research**:\n",
    "    * **Description A2-level**:\n",
    "        * Beurteulungsraster A2 (https://www.bmbwf.gv.at/dam/jcr:ec4e3c97-8d45-4c05-a90d-8d9a764eed7e/reifepr_ahs_mslf_bwr.pdf)\n",
    "        * Kompetenzbeschreibungen für die zweite lebende Fremdsprache: Französisch, Italienisch, Spanisch – A2 (https://www.matura.gv.at/downloads/download/Kompetenzbeschreibungen%20f%C3%BCr%20die%20zweite%20lebende%20Fremdsprache:%20Franz%C3%B6sisch,%20Italienisch,%20Spanisch%20%E2%80%93%20A2)\n",
    "    * **Description B1-level**:\n",
    "        * Bewertungsraster B1 und Begleitmaterialien (überarbeitete Versionen 2023, zu verwenden ab Herbsttermin 2024) (https://www.matura.gv.at/downloads/download/Bewertungsraster%20B1%20und%20Begleitmaterialien%20%28%C3%BCberarbeitete%20Versionen%202023,%20zu%20verwenden%20ab%20Herbsttermin%202024%29)\n",
    "            * Bewertungsraster B1\n",
    "            * Begleittext B1\n",
    "            * Kommentierte Schreibperformanz Spanisch B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cf360af-ffc3-4df9-8316-dc999ddedc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correo electronico \n",
      "\n",
      "De: adrian123@gmail.com  \n",
      "Para: padres123@gmail.com  \n",
      "Fecha: 12 de novembre de 2024  \n",
      "Asunto: me ciudad favorita  \n",
      "\n",
      "Queridos padres,  \n",
      "\n",
      "Estoy en mi ciuadad favorita, Barcelona. (Estoy en mi ciudad favorita, Barcelona.) La ciudad es muy bonita y hay mucha gente viviendo aquí. Además, hace mucho calor aquí en Barcelona.  \n",
      "\n",
      "Hoy he visto la ciudad. Para el almuerzo probe tapas en un pequeno restaurante local. (Para el almuerzo probé tapas en un pequeño restaurante local.) ! Fue delicioso! Me gusta de la playa y comer. (Me gusta la playa y comer.) Luego pasee por el parque y pasee por la playa de la noche. (Luego paseé por el parque y paseé por la playa de la noche.) Mas tarde me relaje en la playa de la Barcelona. (Más tarde me relajé en la playa de Barcelona.)  \n",
      "\n",
      "Manana tengo planes emocionantes. (Mañana tengo planes emocionantes.) Quiero visitar muchos mercados y restaurantes para probar mas comida tipica. (Quiero visitar muchos mercados y restaurantes para probar más comida típica.) Tambien espero encontar algunos Souvenirs unicos para llevar a casa. (También espero encontrar algunos souvenirs únicos para llevar a casa.)  \n",
      "\n",
      "Estoy disfrutando mucho de mi tiempo aqui y no puedo esperar para contarles mas. (Estoy disfrutando mucho de mi tiempo aquí y no puedo esperar para contarles más.)  \n",
      "\n",
      "Saludos,  \n",
      "Adrian  \n",
      "\n",
      "Feedback: Du hast viele gute Ideen, aber achte auf die richtige Schreibweise und die Konjugation der Verben.  \n",
      "An diesem Grammatikthema/diesen Grammatikthemen solltest du noch arbeiten: Rechtschreibung, Verbkonjugation, Akzentzeichen.  \n",
      "Beurteilungsraster A2:  \n",
      "- Erfüllung der Aufgabenstellung (EA): gut, du hast das Thema gut behandelt.  \n",
      "- Aufbau und Layout (AL): gut, der Aufbau ist klar und verständlich.  \n",
      "- Spektrum Sprachlicher Mittel (SSM): gut, du verwendest verschiedene Sätze.  \n",
      "- Sprachrichtigkeit (SR): befriedigend, es gibt einige Fehler in der Grammatik und Rechtschreibung.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Correct the mistakes of the provided text. The text was written by a student. The student should have a language level A2-B1 according\n",
    "to Common European Framework of Reference for Languages: Learning, Teaching, Assessment CEFRL. Correct the mistakes and provide the adequate solutions.\n",
    "Don't change the original input text. Only use this structure to correct the text:\n",
    "\n",
    "Example 1: \"Yo te llama José.\"\n",
    "Corrected example 1: \"Yo te llama José. (Yo me llamo José.)\"\n",
    "Example 2: \"Las coches es caros.\"\n",
    "Corrected example 2: \"Las coches es caros. (Las coches son caros.)\"\n",
    "Example 3: \"Hola María, yo te llama José.\"\n",
    "Corrected example 3: \"Hola María, yo te llama José. (Hola María, me llamo José.) Me gusta los caballo. (Me gustan los caballos.)\"\n",
    "\n",
    "Please correct this text and provide one sentence objective and simple with feedback and which grammar topics the student should improve.\n",
    "The feedback and explanations have to be in german.\n",
    "Feedback structure:\n",
    "\"Feedback: [insert feedback sentence here]\n",
    "An diesem Grammatikthema/diesen Grammatikthemen solltest du noch arbeiten: [insert grammar topics here]\n",
    "Beurteilungsraster A2:\n",
    "- Erfüllung der Aufgabenstellung (EA): [insert chosen descriptions]\n",
    "- Aufbau und Layout (AL): [insert chosen descriptions]\n",
    "- Spektrum Sprachlicher Mittel (SSM): [insert chosen descriptions]\n",
    "- Sprachrichtigkeit (SR): [insert chosen descriptions]\"\n",
    "```{document_text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10fa85-4e8d-4b4c-8988-8ea1f798e785",
   "metadata": {},
   "source": [
    "## Llamaindex: RAG-pipeline\n",
    "### Ad data to the pipeline\n",
    "* This data by the **Ministry for Education, Science and Research** was used to create a `VectorStoreIndex`. The data is stored in the folder `./RAG_data`:\n",
    "    * **Description A2-level**:\n",
    "        * Beurteilungsraster A2 (https://www.bmbwf.gv.at/dam/jcr:ec4e3c97-8d45-4c05-a90d-8d9a764eed7e/reifepr_ahs_mslf_bwr.pdf)\n",
    "        * Kompetenzbeschreibungen für die zweite lebende Fremdsprache: Französisch, Italienisch, Spanisch – A2 (https://www.matura.gv.at/downloads/download/Kompetenzbeschreibungen%20f%C3%BCr%20die%20zweite%20lebende%20Fremdsprache:%20Franz%C3%B6sisch,%20Italienisch,%20Spanisch%20%E2%80%93%20A2)\n",
    "    * **Description B1-level**:\n",
    "        * Bewertungsraster B1 und Begleitmaterialien (überarbeitete Versionen 2023, zu verwenden ab Herbsttermin 2024) (https://www.matura.gv.at/downloads/download/Bewertungsraster%20B1%20und%20Begleitmaterialien%20%28%C3%BCberarbeitete%20Versionen%202023,%20zu%20verwenden%20ab%20Herbsttermin%202024%29)\n",
    "            * Bewertungsraster B1\n",
    "            * Begleittext B1\n",
    "            * Kommentierte Schreibperformanz Spanisch B1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a01c5b-7d08-4bff-9373-9a759632dd15",
   "metadata": {},
   "source": [
    "**Update your packages to use the pipeline!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a61049-73fc-4f6b-a736-d3349b0de612",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60916fa3-1073-4f83-ba69-a8d02c3be770",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-core in c:\\users\\faufr\\anaconda3\\lib\\site-packages (0.12.14)\n",
      "Requirement already satisfied: llama-index-llms-openai in c:\\users\\faufr\\anaconda3\\lib\\site-packages (0.3.14)\n",
      "Requirement already satisfied: llama-index-embeddings-openai in c:\\users\\faufr\\anaconda3\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: llama-index-readers-file in c:\\users\\faufr\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (4.67.1)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (1.0.8)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (11.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (1.24.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (2.32.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (0.8.0)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (3.9.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (3.11.10)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (3.2.1)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (0.2.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (4.12.2)\n",
      "Requirement already satisfied: httpx in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (0.27.2)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (8.3.0)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (2.0.37)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (1.2.15)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (2024.12.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (2.10.3)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-core) (1.12.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-llms-openai) (1.60.1)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-readers-file) (4.12.3)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-readers-file) (0.0.26)\n",
      "Requirement already satisfied: pandas in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-readers-file) (1.4.2)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from llama-index-readers-file) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.3.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (4.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.2.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (5.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.18.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (2022.3.15)\n",
      "Requirement already satisfied: sniffio in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (1.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (1.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (3.5.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai) (0.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->llama-index-llms-openai) (3.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from httpx->llama-index-core) (1.0.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from httpx->llama-index-core) (2024.12.14)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core) (0.4.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core) (0.4.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from dataclasses-json->llama-index-core) (3.23.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from pandas->llama-index-readers-file) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from pandas->llama-index-readers-file) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\faufr\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->llama-index-readers-file) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-core llama-index-llms-openai llama-index-embeddings-openai llama-index-readers-file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca1847-4727-46a4-8486-1f5d35b01a83",
   "metadata": {},
   "source": [
    "### Creating a VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d80494d-6c77-4c21-87bc-670558e055b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new index from ./RAG_data\n",
      "Successfully loaded: bist_lfs_kompetenzbeschreibungen_fr-it-sp_A2_2012-09-17.pdf\n",
      "Successfully loaded: reifepr_ahs_mslf_bwr.pdf\n",
      "Successfully loaded: srdp_lfs_Bewertungsraster_B1_2023.pdf\n",
      "Successfully loaded: srdp_lfs_Bewertungsraster_B1_Begleittext_2023.pdf\n",
      "Successfully loaded: srdp_lfs_Spanisch_B1_kommentierte_Schreibperformanzen_2023.pdf\n",
      "Loaded 187 PDF document chunks.\n",
      "Index persisted to: ./RAG_index\n",
      "\n",
      "Answer: The PDFs contain competency descriptions for French, Italian, and Spanish at level A2.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# 1. Load environment variables from .env\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 2. Set your OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# 3. Import LlamaIndex components\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    "    StorageContext\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.readers.file import PDFReader\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Configuration parameters\n",
    "EMBED_MODEL = \"text-embedding-ada-002\"\n",
    "LLM_MODEL = \"gpt-3.5-turbo\"\n",
    "TEMPERATURE = 0\n",
    "\n",
    "def build_pdf_index(pdf_folder: str, persist_dir: Optional[str] = None):\n",
    "    # Initialize PDF reader\n",
    "    pdf_loader = PDFReader()\n",
    "    \n",
    "    try:\n",
    "        # 1. Gather PDF files\n",
    "        folder_path = Path(pdf_folder)\n",
    "        all_documents = []\n",
    "        for pdf_file in folder_path.glob(\"*.pdf\"):\n",
    "            try:\n",
    "                docs = pdf_loader.load_data(file=pdf_file)\n",
    "                all_documents.extend(docs)\n",
    "                print(f\"Successfully loaded: {pdf_file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {pdf_file.name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Loaded {len(all_documents)} PDF document chunks.\")\n",
    "\n",
    "        if not all_documents:\n",
    "            raise ValueError(\"No documents were successfully loaded\")\n",
    "\n",
    "        # 2. Configure global settings\n",
    "        Settings.llm = OpenAI(\n",
    "            model=LLM_MODEL,\n",
    "            temperature=TEMPERATURE,\n",
    "            api_key=openai.api_key\n",
    "        )\n",
    "        Settings.embed_model = OpenAIEmbedding(\n",
    "            model=EMBED_MODEL,\n",
    "            api_key=openai.api_key\n",
    "        )\n",
    "\n",
    "        # 3. Create the vector store index\n",
    "        index = VectorStoreIndex.from_documents(documents=all_documents)\n",
    "\n",
    "        # 4. Persist the index\n",
    "        if persist_dir:\n",
    "            index.storage_context.persist(persist_dir=persist_dir)\n",
    "            print(f\"Index persisted to: {persist_dir}\")\n",
    "\n",
    "        return index\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error building index: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        persist_path = \"./RAG_index\"\n",
    "        pdf_folder = \"./RAG_data\"\n",
    "        \n",
    "        # Build and persist the index\n",
    "        print(f\"Building new index from {pdf_folder}\")\n",
    "        index = build_pdf_index(pdf_folder=pdf_folder, persist_dir=persist_path)\n",
    "\n",
    "        # Create a query engine and test it\n",
    "        query_engine = index.as_query_engine(\n",
    "            response_mode=\"compact\",\n",
    "            similarity_top_k=3\n",
    "        )\n",
    "\n",
    "        # Run a test query\n",
    "        user_query = \"Please summarize all the key points in these PDFs.\"\n",
    "        response = query_engine.query(user_query)\n",
    "        print(\"\\nAnswer:\", response)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in main: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42323650-0691-4768-8ff9-4a1214920355",
   "metadata": {},
   "source": [
    "### Use the chatbot to ask questions about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98ad2a48-a037-432f-b1d8-7fddc1af952c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Welcome to the Interactive RAG Query System\n",
      "Type 'exit' to end the session\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your question:  About which levels of spanish to do have information?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing your query...\n",
      "\n",
      "Answer: The information available pertains to the A2 level of Spanish.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your question:  Is there also information about B1?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing your query...\n",
      "\n",
      "Answer: Yes, there is information about B1. It outlines skills such as the ability to write simple, coherent texts on familiar topics, interact in writing by conveying simple, immediate information, and express personal importance. At this level, individuals can recount uncomplicated stories or descriptions fluently, and share thoughts on both abstract and concrete topics. They can also write reports and essays on topics of general interest, using simple language to list pros and cons, express and justify their own opinions. For vocabulary, B1 level individuals show a good command of basic vocabulary but may still make elementary mistakes when expressing more complex matters or unfamiliar topics. They can use a wide range of simple words appropriately when discussing familiar topics. As for grammatical correctness, they can communicate adequately in familiar situations and use a repertoire of frequently used phrases and expressions correctly. They can also give simple information of immediate importance and express the essence of what they want to say in an understandable manner. Regarding spelling mastery, they can write coherently, and their spelling, punctuation, and layout are precise enough to be mostly understandable.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your question:  Exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending session. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# 1. Load environment variables from .env\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 2. Set your OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# 3. Import LlamaIndex components\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Configuration parameters - updated to use GPT-4\n",
    "EMBED_MODEL = \"text-embedding-ada-002\"\n",
    "LLM_MODEL = \"gpt-4\"  # or \"gpt-4-turbo-preview\" for the latest version\n",
    "TEMPERATURE = 0.7    # Increased for more creative responses\n",
    "\n",
    "def load_and_query_index(persist_dir: str, query: str):\n",
    "    try:\n",
    "        # Configure global settings with GPT-4\n",
    "        Settings.llm = OpenAI(\n",
    "            model=LLM_MODEL,\n",
    "            temperature=TEMPERATURE,\n",
    "            api_key=openai.api_key\n",
    "        )\n",
    "        Settings.embed_model = OpenAIEmbedding(\n",
    "            model=EMBED_MODEL,\n",
    "            api_key=openai.api_key\n",
    "        )\n",
    "\n",
    "        # Load the existing index\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "        index = load_index_from_storage(storage_context=storage_context)\n",
    "\n",
    "        # Create an advanced query engine with additional parameters\n",
    "        query_engine = index.as_query_engine(\n",
    "            response_mode=\"tree_summarize\",  # Provides more structured responses\n",
    "            similarity_top_k=5,              # Increased for broader context\n",
    "            streaming=True                   # Enable streaming responses\n",
    "        )\n",
    "\n",
    "        # Execute the query\n",
    "        response = query_engine.query(query)\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying index: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def interactive_query_session():\n",
    "    persist_path = \"./RAG_index\"\n",
    "    \n",
    "    print(\"\\nWelcome to the Interactive RAG Query System\")\n",
    "    print(\"Type 'exit' to end the session\")\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_query = input(\"\\nEnter your question: \").strip()\n",
    "            \n",
    "            # Check for exit command\n",
    "            if user_query.lower() == 'exit':\n",
    "                print(\"Ending session. Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            # Process query and get response\n",
    "            if user_query:\n",
    "                print(\"\\nProcessing your query...\")\n",
    "                response = load_and_query_index(persist_path, user_query)\n",
    "                print(\"\\nAnswer:\", response)\n",
    "            else:\n",
    "                print(\"Please enter a valid query.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            print(\"Please try again with a different query.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        interactive_query_session()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nSession terminated by user. Goodbye!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d4af173-f8d5-4604-8f27-bd4e31a4d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main themes discussed in these documents are the assessment and grading of written performances in Spanish. The documents detail the evaluation of tasks that involve writing informative articles for a young audience interested in intercultural exchange. The evaluation criteria include the fulfillment of the task, coherence and cohesion of the text, the use of appropriate language and style, and the treatment of specific content points. The documents also address issues such as the appropriateness of the title and introduction, the balance of content points, and the clarity of the writer's role.\n"
     ]
    }
   ],
   "source": [
    "# You can also use it programmatically:\n",
    "persist_path = \"./RAG_index\"\n",
    "query = \"What are the main themes discussed in these documents?\"\n",
    "response = load_and_query_index(persist_path, query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ea1462-ab6c-422b-89e6-6692e160c1a8",
   "metadata": {},
   "source": [
    "## Now have a look at the finished version stored in the Jupyter-Notebook `FAST_VERSION.ipynb`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bfd349-762a-4c79-8497-95d9393c76f4",
   "metadata": {},
   "source": [
    "### 2nd RAG-pipeline for task-creation\n",
    "* The collection of tasks was privatly assembled and includes grammar exercises. These exercises were used to create a `VectorStoreIndex`. The data is stored in the folder `./TASK_RAG_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa70ee-8475-4baa-a61d-d23d0bc467b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD second vectorestoreindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f0e32-bae9-4fd5-b1ab-a54ec26a2c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD task creation prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41575f-a474-46d9-bfe4-b2bec5a3d40c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
